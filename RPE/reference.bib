@INPROCEEDINGS{abran_maintenance_1991,
author={Abran, A. and Nguyenkim, H.},
title={Analysis of maintenance work categories through measurement},
booktitle={Software Maintenance, 1991., Proceedings. Conference on},
year={1991},
pages={104-113},
keywords={DP management;administrative data processing;software maintenance;software metrics;Canadian organization;daily data-collection process;empirical data;improved measurement program;maintenance environment;maintenance process;maintenance work categories;opinion surveys;overall workload distribution;two-year measurement effort;work requests;Application software;Area measurement;Computer applications;Gain measurement;Management information systems;Portfolios;Productivity;Software maintenance;Software measurement;Software systems},
doi={10.1109/ICSM.1991.160315},}

@inproceedings{ramesh_implementing_1995,
 author = {Ramesh, B. and Powers, T. and Stubbs, C. and Edwards, M.},
 title = {Implementing requirements traceability: a case study},
 booktitle = {Proceedings of the Second IEEE International Symposium on Requirements Engineering},
 series = {RE '95},
 year = {1995},
 isbn = {0-8186-7017-7},
 pages = {89--},
 url = {http://dl.acm.org/citation.cfm?id=827254.827798},
 acmid = {827798},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {computer aided software engineering, military computing, requirements traceability, software standards, software tools, standards, systems analysis, systems development efforts, systems development organization},
}

@article{lindvall_practical_1996,
 author = {Lindvall, Mikael and Sandahl, Kristian},
 title = {Practical implications of traceability},
 journal = {Softw. Pract. Exper.},
 issue_date = {Oct. 1996},
 volume = {26},
 number = {10},
 month = oct,
 year = {1996},
 issn = {0038-0644},
 pages = {1161--1180},
 numpages = {20},
 url = {http://dx.doi.org/10.1002/(SICI)1097-024X(199610)26:10<1161::AID-SPE58>3.3.CO;2-O},
 doi = {10.1002/(SICI)1097-024X(199610)26:10<1161::AID-SPE58>3.3.CO;2-O},
 acmid = {241708},
 publisher = {John Wiley \& Sons, Inc.},
 address = {New York, NY, USA},
 keywords = {analysis of object models, case-study, object-oriented analysis and design, objectory, traceability},
}

@inproceedings{gotel_extended_1997,
 author = {Gotel, Orlena and Finkelsteiin, Anthony},
 title = {Extended Requirements Traceability: Results of an Industrial Case Study},
 booktitle = {Proceedings of the 3rd IEEE International Symposium on Requirements Engineering},
 series = {RE '97},
 year = {1997},
 isbn = {0-8186-7740-6},
 pages = {169--},
 url = {http://dl.acm.org/citation.cfm?id=827255.827842},
 acmid = {827842},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
}

@inproceedings{chou_os_2001,
 author = {Chou, Andy and Yang, Junfeng and Chelf, Benjamin and Hallem, Seth and Engler, Dawson},
 title = {An empirical study of operating systems errors},
 booktitle = {Proceedings of the eighteenth ACM symposium on Operating systems principles},
 series = {SOSP '01},
 year = {2001},
 isbn = {1-58113-389-8},
 location = {Banff, Alberta, Canada},
 pages = {73--88},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/502034.502042},
 doi = {10.1145/502034.502042},
 acmid = {502042},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@ARTICLE{antoniol_recoveringtraceability_2002,
    author = {Giuliano Antoniol and Gerardo Canfora and Gerardo Casazza and Andrea De Lucia and Ettore Merlo},
    title = {Recovering traceability links between code and documentation},
    journal = {IEEE Trans. Softw. Eng},
    year = {2002},
    pages = {970--983}
}

@inproceedings{swift_os_2003,
 author = {Swift, Michael M. and Bershad, Brian N. and Levy, Henry M.},
 title = {Improving the reliability of commodity operating systems},
 booktitle = {Proceedings of the nineteenth ACM symposium on Operating systems principles},
 series = {SOSP '03},
 year = {2003},
 isbn = {1-58113-757-5},
 location = {Bolton Landing, NY, USA},
 pages = {207--222},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/945445.945466},
 doi = {10.1145/945445.945466},
 acmid = {945466},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {I/O, device drivers, protection, recovery, virtual memory},
}

@inproceedings{marcus_recovering_2003,
 author = {Marcus, Andrian and Maletic, Jonathan I.},
 title = {Recovering documentation-to-source-code traceability links using latent semantic indexing},
 booktitle = {Proceedings of the 25th International Conference on Software Engineering},
 series = {ICSE '03},
 year = {2003},
 isbn = {0-7695-1877-X},
 location = {Portland, Oregon},
 pages = {125--135},
 numpages = {11},
 url = {http://dl.acm.org/citation.cfm?id=776816.776832},
 acmid = {776832},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
}

@INPROCEEDINGS{spanoudakis_software_2004,
    author = {George Spanoudakis and Andrea Zisman},
    title = {Software Traceability: A Roadmap},
    booktitle = {Handbook of Software Engineering and Knowledge Engineering},
    year = {2004},
    pages = {395--428},
    publisher = {World Scientific Publishing}
}

@article{hayes_advancing_2006,
 author = {Hayes, Jane Huffman and Dekhtyar, Alex and Sundaram, Senthil Karthikeyan},
 title = {Advancing Candidate Link Generation for Requirements Tracing: The Study of Methods},
 journal = {IEEE Trans. Softw. Eng.},
 issue_date = {January 2006},
 volume = {32},
 number = {1},
 month = jan,
 year = {2006},
 issn = {0098-5589},
 pages = {4--19},
 numpages = {16},
 url = {http://dx.doi.org/10.1109/TSE.2006.3},
 doi = {10.1109/TSE.2006.3},
 acmid = {1112799},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {Independent Validation and Verification (IV\&V), LSI, Requirements tracing, Requirements tracing, dynamic link generation, Verification and Validation (V\&V), Independent Validation and Verification (IV\&V), information retrieval, TF-IDF, LSI, recall, precision., TF-IDF, Verification and Validation (V\&\&V), dynamic link generation, information retrieval, precision., recall},
}

@book{lawrence_software_2006,
  title={Software engineering: theory and practice},
  author={Lawrence, Pfleeger Shari and Pfleeger, Shari Lawrence and Atlee, Joanne M},
  year={2006},
  publisher={Pearson Education India}
}

@inproceedings{neumuller_case_2006,
 author = {Neumuller, Christian and Grunbacher, Paul},
 title = {Automating Software Traceability in Very Small Companies: A Case Study and Lessons Learne},
 booktitle = {Proceedings of the 21st IEEE/ACM International Conference on Automated Software Engineering},
 series = {ASE '06},
 year = {2006},
 isbn = {0-7695-2579-2},
 pages = {145--156},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/ASE.2006.25},
 doi = {10.1109/ASE.2006.25},
 acmid = {1169314},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
}

@inproceedings{deLucia_incremental_2006,
 author = {De Lucia, Andrea and Oliveto, Rocco and Sgueglia, Paola},
 title = {Incremental Approach and User Feedbacks: a Silver Bullet for Traceability Recovery},
 booktitle = {Proceedings of the 22nd IEEE International Conference on Software Maintenance},
 series = {ICSM '06},
 year = {2006},
 isbn = {0-7695-2354-4},
 pages = {299--309},
 numpages = {11},
 url = {http://dx.doi.org/10.1109/ICSM.2006.32},
 doi = {10.1109/ICSM.2006.32},
 acmid = {1173010},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
}

@inproceedings{asuncion_an_2007,
 author = {Asuncion, Hazeline U. and Fran\c{c}ois, Fr{\'e}d{\'e}ric and Taylor, Richard N.},
 title = {An end-to-end industrial software traceability tool},
 booktitle = {Proceedings of the the 6th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on The foundations of software engineering},
 series = {ESEC-FSE '07},
 year = {2007},
 isbn = {978-1-59593-811-4},
 location = {Dubrovnik, Croatia},
 pages = {115--124},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1287624.1287642},
 doi = {10.1145/1287624.1287642},
 acmid = {1287642},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {end-to-end software traceability, process traceability, requirements traceability},
}

@inproceedings{jiang_incremental_2008,
 author = {Hsin-Yi Jiang and Nguyen, T. N. and Ing-Xiang Chen and Jaygarl, H. and Chang, C. K.},
 title = {Incremental Latent Semantic Indexing for Automatic Traceability Link Evolution Management},
 booktitle = {Proceedings of the 2008 23rd IEEE/ACM International Conference on Automated Software Engineering},
 series = {ASE '08},
 year = {2008},
 isbn = {978-1-4244-2187-9},
 pages = {59--68},
 numpages = {10},
 url = {http://dx.doi.org/10.1109/ASE.2008.16},
 doi = {10.1109/ASE.2008.16},
 acmid = {1642943},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {software evolution, incremental latent semantic indexing, automatic traceability link evolution management, software artifacts},
}

@Misc{driver_wiki,
  author =       "wikipedia",
  year =         "2013",
  title =        "Device driver",
  note =         "",
  url =           "http://en.wikipedia.org/wiki/Device_driver",
  howpublished = "",
  month = 	 april,
  lastaccessed = "April 21, 2013",
}



@Misc{dio,
  author =       "Sealevel",
  year =         "2009",
  title =        "Low Profile PCI 24 Channel TTL Digital Interface",
  note =         "",
  url =           "http://www.sealevel.com/store/8018-low-profile-pci-24-channel-ttl-digital-interface.html",
  howpublished = "",
  month = 	 mar,
  lastaccessed = "March 13, 2013",
}

@Misc{e100,
  author =       "Intel",
  year =         "2006",
  title =        "Intel 8255x 10/100 Mbps Ethernet Controller Family",
  note =         "",
  url =           "http://www.intel.com/design/network/manuals/8255X_OpenSDM.htm",
  howpublished = "",
  month = 	 jan,
  lastaccessed = "March 3, 2010",
}

@inproceedings{bellard_qemu_2005,
 author = {Bellard, Fabrice},
 title = {QEMU, a fast and portable dynamic translator},
 booktitle = {Proceedings of the annual conference on USENIX Annual Technical Conference},
 series = {ATEC '05},
 year = {2005},
 location = {Anaheim, CA},
 pages = {41--41},
 numpages = {1},
 url = {http://dl.acm.org/citation.cfm?id=1247360.1247401},
 acmid = {1247401},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA},
}

@Misc{bellard_qemuwiki_2013,
  author =       "Bellard, Fabrice",
  year =         "2013",
  title =        "QEMU",
  note =         "",
  url =           "http://wiki.qemu.org/Main_Page",
  howpublished = "",
  month = 	 jan,
  lastaccessed = "April 22, 2013",
}

@inproceedings{polikarpova_comparative_2009,
	address = {New York, {NY}, {USA}},
	series = {{ISSTA} '09},
	title = {A comparative study of programmer-written and automatically inferred contracts},
	isbn = {978-1-60558-338-9},
	url = {http://doi.acm.org/10.1145/1572272.1572284},
	doi = {10.1145/1572272.1572284},
	abstract = {Where do contracts - specification elements embedded in executable code - come from? To produce them, should we rely on the programmers, on automatic tools, or some combination? Recent work, in particular the Daikon system, has shown that it is possible to infer some contracts automatically from program executions. The main incentive has been an assumption that most programmers are reluctant to invent the contracts themselves. The experience of contract-supporting languages, notably Eiffel, disproves that assumption: programmers will include contracts if given the right tools. That experience also shows, however, that the resulting contracts are generally partial and occasionally incorrect. Contract inference tools provide the opportunity for studying objectively the quality of programmer-written contracts, and for assessing the respective roles of humans and tools. Working on 25 classes taken from different sources such as widely-used standard libraries and code written by students, we applied Daikon to infer contracts and compared the results (totaling more than 19500 inferred assertion clauses) with the already present contracts. We found that a contract inference tool can be used to strengthen programmer-written contracts, but cannot infer all contracts that humans write. The tool generates around five times as many relevant assertion clauses as written by programmers; but it only finds around 60\% of those originally written by programmers. Around a third of the generated assertions clauses are either incorrect or irrelevant. The study also uncovered interesting correlations between the quality of inferred contracts and some code metrics.},
	urldate = {2013-01-28},
	booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
	publisher = {{ACM}},
	author = {Polikarpova, Nadia and Ciupa, Ilinca and Meyer, Bertrand},
	year = {2009},
	keywords = {dynamic contract inference, eiffel},
	pages = {93–104},
	annote = {This paper uses Daikon system to generate specifications automatically, and compares the results with the existing human written contracts. The result shows that although Daikon produced five times as many contracts as the human written ones, it only produce 60\% of the human written contracts. The other contracts generated are either irrelevant or incorrect. As a result, the paper suggests to use tools as a complement and strengthen the programmer-written contracts, other than using tools only.},
	file = {ACM Full Text PDF:/home/derek/.zotero/zotero/6srlskv9.default/zotero/storage/69WBUREE/Polikarpova et al. - 2009 - A comparative study of programmer-written and auto.pdf:application/pdf}
},

@inproceedings{xiao_automated_2012,
	address = {New York, {NY}, {USA}},
	series = {{FSE} '12},
	title = {Automated extraction of security policies from natural-language software documents},
	isbn = {978-1-4503-1614-9},
	url = {http://doi.acm.org/10.1145/2393596.2393608},
	doi = {10.1145/2393596.2393608},
	abstract = {Access Control Policies ({ACP)} specify which principals such as users have access to which resources. Ensuring the correctness and consistency of {ACPs} is crucial to prevent security vulnerabilities. However, in practice, {ACPs} are commonly written in Natural Language ({NL)} and buried in large documents such as requirements documents, not amenable for automated techniques to check for correctness and consistency. It is tedious to manually extract {ACPs} from these {NL} documents and validate {NL} functional requirements such as use cases against {ACPs} for detecting inconsistencies. To address these issues, we propose an approach, called {Text2Policy}, to automatically extract {ACPs} from {NL} software documents and resource-access information from {NL} scenario-based functional requirements. We conducted three evaluations on the collected {ACP} sentences from publicly available sources along with use cases from both open source and proprietary projects. The results show that {Text2Policy} effectively identifies {ACP} sentences with the precision of 88.7\% and the recall of 89.4\%, extracts {ACP} rules with the accuracy of 86.3\%, and extracts action steps with the accuracy of 81.9\%.},
	urldate = {2013-01-28},
	booktitle = {Proceedings of the {ACM} {SIGSOFT} 20th International Symposium on the Foundations of Software Engineering},
	publisher = {{ACM}},
	author = {Xiao, Xusheng and Paradkar, Amit and Thummalapenta, Suresh and Xie, Tao},
	year = {2012},
	keywords = {access control, natural language processing, requirements analysis},
	pages = {12:1–12:11},
	annote = {This paper presents an approach called {Text2Policy} to automatically extract Access Control Policies ({ACP)} from Natural Language ({NL)} requirement documents. The approach consists of two steps. First, identify {ACP} sentences. Second, generate {ACP} rules. Experiment results show effectiveness within both steps.},
	file = {ACM Full Text PDF:/home/derek/.zotero/zotero/6srlskv9.default/zotero/storage/G233H8R6/Xiao et al. - 2012 - Automated extraction of security policies from nat.pdf:application/pdf}
},

@inproceedings{nimmer_automatic_2002,
	address = {New York, {NY}, {USA}},
	series = {{ISSTA} '02},
	title = {Automatic generation of program specifications},
	isbn = {1-58113-562-9},
	url = {http://doi.acm.org/10.1145/566172.566213},
	doi = {10.1145/566172.566213},
	abstract = {Producing specifications by dynamic (runtime) analysis of program executions is potentially unsound, because the analyzed executions may not fully characterize all possible executions of the program. In practice, how accurate are the results of a dynamic analysis? This paper describes the results of an investigation into this question, determining how much specifications generalized from program runs must be changed in order to be verified by a static checker. Surprisingly, small test suites captured nearly all program behavior required by a specific type of static checking; the static checker guaranteed that the implementations satisfy the generated specifications, and ensured the absence of runtime exceptions. Measured against this verification task, the generated specifications scored over 90\% on precision, a measure of soundness, and on recall, a measure of {completeness.This} is a positive result for testing, because it suggests that dynamic analyses can capture all semantic information of interest for certain applications. The experimental results demonstrate that a specific technique, dynamic invariant detection, is effective at generating consistent, sufficient specifications for use by a static checker. Finally, the research shows that combining static and dynamic analyses over program specifications has benefits for users of each technique, guaranteeing soundness of the dynamic analysis and lessening the annotation burden for users of the static analysis.},
	urldate = {2013-01-28},
	booktitle = {Proceedings of the 2002 {ACM} {SIGSOFT} international symposium on Software testing and analysis},
	publisher = {{ACM}},
	author = {Nimmer, Jeremy W. and Ernst, Michael D.},
	year = {2002},
	pages = {229–239},
	annote = {This paper proposes a technique that dynamically instruments the program to get the invariant of the program, and infer the specification for the program. The author also uses a static checker to ensure the correctness of the specification. The experiment results show that the dynamic invariant detector can generate specifications precisely enough to express the semantic of the program.},
	file = {ACM Full Text PDF:/home/derek/.zotero/zotero/6srlskv9.default/zotero/storage/UD3ZXK8M/Nimmer and Ernst - 2002 - Automatic generation of program specifications.pdf:application/pdf}
},

@inproceedings{tan_/*icomment:_2007,
	address = {New York, {NY}, {USA}},
	series = {{SOSP} '07},
	title = {/*icomment: bugs or bad comments?*/},
	isbn = {978-1-59593-591-5},
	shorttitle = {/*icomment},
	url = {http://doi.acm.org/10.1145/1294261.1294276},
	doi = {10.1145/1294261.1294276},
	abstract = {Commenting source code has long been a common practice in software development. Compared to source code, comments are more direct, descriptive and easy-to-understand. Comments and sourcecode provide relatively redundant and independent information regarding a program's semantic behavior. As software evolves, they can easily grow out-of-sync, indicating two problems: (1) bugs -the source code does not follow the assumptions and requirements specified by correct program comments; (2) bad comments - comments that are inconsistent with correct code, which can confuse and mislead programmers to introduce bugs in subsequent versions. Unfortunately, as most comments are written in natural language, no solution has been proposed to automatically analyze commentsand detect inconsistencies between comments and source code. This paper takes the first step in automatically analyzing commentswritten in natural language to extract implicit program rulesand use these rules to automatically detect inconsistencies between comments and source code, indicating either bugs or bad comments. Our solution, {iComment}, combines Natural Language Processing({NLP)}, Machine Learning, Statistics and Program Analysis techniques to achieve these goals. We evaluate {iComment} on four large code bases: Linux, Mozilla, Wine and Apache. Our experimental results show that {iComment} automatically extracts 1832 rules from comments with 90.8-100\% accuracy and detects 60 comment-code inconsistencies, 33 newbugs and 27 bad comments, in the latest versions of the four programs. Nineteen of them (12 bugs and 7 bad comments) have already been confirmed by the corresponding developers while the others are currently being analyzed by the developers.},
	urldate = {2013-01-29},
	booktitle = {Proceedings of twenty-first {ACM} {SIGOPS} symposium on Operating systems principles},
	publisher = {{ACM}},
	author = {Tan, Lin and Yuan, Ding and Krishna, Gopal and Zhou, Yuanyuan},
	year = {2007},
	keywords = {comment analysis, natural language processing for software engineering, programming rules and static analysis},
	pages = {145–158},
	annote = {This is the first paper to analyze the natural language comments of the code, extract implicit rules, and detect inconsistencies between the comments and the codes. The author adopts Natural Language Processing ({NLP)}, Machine Learning, Statistics and Program Analysis technique to do these jobs automatically. Experiment on four large software bases shows that this approach is efficient.},
	file = {ACM Full Text PDF:/home/derek/.zotero/zotero/6srlskv9.default/zotero/storage/KJCDTE2D/Tan et al. - 2007 - icomment bugs or bad comments.pdf:application/pdf}
},


@inproceedings{zhong_inferring_2009,
	address = {Washington, {DC}, {USA}},
	series = {{ASE} '09},
	title = {Inferring Resource Specifications from Natural Language {API} Documentation},
	isbn = {978-0-7695-3891-4},
	url = {http://dx.doi.org/10.1109/ASE.2009.94},
	doi = {10.1109/ASE.2009.94},
	abstract = {Typically, software libraries provide {API} documentation, through which developers can learn how to use libraries correctly. However, developers may still write code inconsistent with {API} documentation and thus introduce bugs, as existing research shows that many developers are reluctant to carefully read {API} documentation. To find those bugs, researchers have proposed various detection approaches based on known specifications. To mine specifications, many approaches have been proposed, and most of them rely on existing client code. Consequently, these mining approaches would fail to mine specifications when client code is not available. In this paper, we propose an approach, called {Doc2Spec}, that infers resource specifications from {API} documentation. For our approach, we implemented a tool and conducted an evaluation on Javadocs of five libraries. The results show that our approach infers various specifications with relatively high precisions, recalls, and F-scores. We further evaluated the usefulness of inferred specifications through detecting bugs in open source projects. The results show that specifications inferred by {Doc2Spec} are useful to detect real bugs in existing projects.},
	urldate = {2013-01-27},
	booktitle = {Proceedings of the 2009 {IEEE/ACM} International Conference on Automated Software Engineering},
	publisher = {{IEEE} Computer Society},
	author = {Zhong, Hao and Zhang, Lu and Xie, Tao and Mei, Hong},
	year = {2009},
	pages = {307–318},
	annote = {This paper proposes an approach to infer resource specifications in the form of automata from the {API} documents in natural language. The author adopts Natural Language Processing ({NLP)} technique as well as Named Entiry Recognition ({NER)} based on Hidden Markov Model ({HMM)} to facilitate their approach. They implement {Doc2Spec}, which only takes Javadoc as its input, to prototype their technique, and shows usefulness in detecting real bugs in existing projects.},
	file = {ACM Full Text PDF:/home/derek/.zotero/zotero/6srlskv9.default/zotero/storage/86V2TSUX/Zhong et al. - 2009 - Inferring Resource Specifications from Natural Lan.pdf:application/pdf}
},

@inproceedings{tan_tcomment:_2012,
	title = {{@tComment:} Testing Javadoc Comments to Detect Comment-Code Inconsistencies},
	shorttitle = {{@tComment}},
	doi = {10.1109/ICST.2012.106},
	abstract = {Code comments are important artifacts in software. Javadoc comments are widely used in Java for {API} specifications. {API} developers write Javadoc comments, and {API} users read these comments to understand the {API}, e.g., reading a Javadoc comment for a method instead of reading the method body. An inconsistency between the Javadoc comment and body for a method indicates either a fault in the body or, effectively, a fault in the comment that can mislead the method callers to introduce faults in their code. We present a novel approach, called {@TCOMMENT}, for testing Javadoc comments, specifically method properties about null values and related exceptions. Our approach consists of two components. The first component takes as input source files for a Java project and automatically analyzes the English text in Javadoc comments to infer a set of likely properties for a method in the files. The second component generates random tests for these methods, checks the inferred properties, and reports inconsistencies. We evaluated {@TCOMMENT} on seven open-source projects and found 29 inconsistencies between Javadoc comments and method bodies. We reported 16 of these inconsistencies, and 5 have already been confirmed and fixed by the developers.},
	booktitle = {2012 {IEEE} Fifth International Conference on Software Testing, Verification and Validation ({ICST)}},
	author = {Tan, Shin Hwei and Marinov, D. and Tan, Lin and Leavens, {G.T.}},
	month = apr,
	year = {2012},
	keywords = {{API} specification, application program interfaces, Arrays, comment analysis, comment-code inconsistency detection, Contracts, Educational institutions, English text analysis, formal specification, Java, Javadoc comment testing, libraries, natural language processing, program testing, random testing, software artifact, Synchronization, {@TCOMMENT}, test generation, Testing, text analysis},
	pages = {260 --269},
	annote = {This paper presents an novel approach called {@tComment} to check the consistency between the Java method comment and the Java method implementation. In this paper, {@tComment} only focuses on null values and exceptions. The author did experiments on seven open source projects, and found 29 inconsistencies.},
	file = {IEEE Xplore Abstract Record:/home/derek/.zotero/zotero/6srlskv9.default/zotero/storage/UDJEU7I3/articleDetails.html:text/html;IEEE Xplore Full Text PDF:/home/derek/.zotero/zotero/6srlskv9.default/zotero/storage/ZM86K6C3/Tan et al. - 2012 - @tComment Testing Javadoc Comments to Detect Comm.pdf:application/pdf}
},

@inproceedings{khalek_testera:_2011,
	title = {{TestEra:} A tool for testing Java programs using alloy specifications},
	shorttitle = {{TestEra}},
	doi = {10.1109/ASE.2011.6100137},
	abstract = {This tool paper presents an embodiment of {TestEra} - a framework developed in previous work for specification-based testing of Java programs. To test a Java method, {TestEra} uses the method's pre-condition specification to generate test inputs and the post-condition to check correctness of outputs. {TestEra} supports specifications written in Alloy - a first-order, declarative language based on relations - and uses the {SAT-based} back-end of the Alloy tool-set for systematic generation of test suites. Each test case is a {JUnit} test method, which performs three key steps: (1) initialization of pre-state, i.e., creation of inputs to the method under test; (2) invocation of the method; and (3) checking the correctness of post-state, i.e., checking the method output. The tool supports visualization of inputs and outputs as object graphs for graphical illustration of method behavior. {TestEra} is available for download to be used as a library or as an Eclipse plug-in.},
	booktitle = {2011 26th {IEEE/ACM} International Conference on Automated Software Engineering ({ASE)}},
	author = {Khalek, {S.A.} and Yang, Guowei and Zhang, Lingming and Marinov, D. and Khurshid, S.},
	month = nov,
	year = {2011},
	keywords = {alloy specification, Alloy tool-set, declarative language, Eclipse plug-in, graphical illustration, graphs, Java, Java program testing, {JUnit} test method, libraries, Metals, object graph, program testing, Receivers, {SAT-based} backend, Software engineering, specification-based testing, specification languages, Systematics, {TestEra}, Testing},
	pages = {608 --611},
	annote = {This is a tool paper, which illustrates {TestEra.} {TestEra} is a tool for testing java programs. It receives specially annotated java program using a first-order specification language Alloy, generates {JUnit} testing code, and runs the testing code to give a testing report.},
	file = {IEEE Xplore Abstract Record:/home/derek/.zotero/zotero/6srlskv9.default/zotero/storage/S9AAJDME/articleDetails.html:text/html;IEEE Xplore Full Text PDF:/home/derek/.zotero/zotero/6srlskv9.default/zotero/storage/AFXIKUZM/Khalek et al. - 2011 - TestEra A tool for testing Java programs using al.pdf:application/pdf}
},

@inproceedings{novick_why_2006,
	address = {New York, {NY}, {USA}},
	series = {{SIGDOC} '06},
	title = {Why don't people read the manual?},
	isbn = {1-59593-523-1},
	url = {http://doi.acm.org/10.1145/1166324.1166329},
	doi = {10.1145/1166324.1166329},
	abstract = {Few users of computer applications seek help from the documentation. This paper reports the results of an empirical study of why this is so and examines how, in real work, users solve their usability problems. Based on in-depth interviews with 25 subjects representing a varied cross-section of users, we find that users do avoid using both paper and online help systems. Few users have paper manuals for the most heavily used applications, but none complained about their lack. Online help is more likely to be consulted than paper manuals, but users are equally likely to report that they solve their problem by asking a colleague or experimenting on their own. Users cite difficulties in navigating the help systems, particularly difficulties in finding useful search terms, and disappointment in the level of explanation found.},
	urldate = {2013-01-28},
	booktitle = {Proceedings of the 24th annual {ACM} international conference on Design of communication},
	publisher = {{ACM}},
	author = {Novick, David G. and Ward, Karen},
	year = {2006},
	keywords = {manuals, online help, problem-solving, usability},
	pages = {11–18},
	annote = {This paper studies the reason for the fact that few people use the document to assist their use of computer applications. The author conducted an interview with a varied cross-section users. The result of the interview shows that people prefer to consult their colleague or experiment by themselves other than referring to paper or online document. The reason for this phenomenon is that the structure of the documents are hard for the users to navigate.},
	file = {ACM Full Text PDF:/home/derek/.zotero/zotero/6srlskv9.default/zotero/storage/47XB9TQC/Novick and Ward - 2006 - Why don't people read the manual.pdf:application/pdf}
}

@inproceedings{Bellard05,
 author = {Bellard, Fabrice},
 title = "{QEMU, a fast and portable dynamic translator}",
booktitle = {Proc. of ATEC, 2005},
}

@inproceedings{Eclipse,
 author = {{Eclipse Foundation}},
 title = "http://www.eclipse.org/",
}
