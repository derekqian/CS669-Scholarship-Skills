
@inproceedings{polikarpova_comparative_2009,
	address = {New York, {NY}, {USA}},
	series = {{ISSTA} '09},
	title = {A comparative study of programmer-written and automatically inferred contracts},
	isbn = {978-1-60558-338-9},
	url = {http://doi.acm.org/10.1145/1572272.1572284},
	doi = {10.1145/1572272.1572284},
	abstract = {Where do contracts - specification elements embedded in executable code - come from? To produce them, should we rely on the programmers, on automatic tools, or some combination? Recent work, in particular the Daikon system, has shown that it is possible to infer some contracts automatically from program executions. The main incentive has been an assumption that most programmers are reluctant to invent the contracts themselves. The experience of contract-supporting languages, notably Eiffel, disproves that assumption: programmers will include contracts if given the right tools. That experience also shows, however, that the resulting contracts are generally partial and occasionally incorrect. Contract inference tools provide the opportunity for studying objectively the quality of programmer-written contracts, and for assessing the respective roles of humans and tools. Working on 25 classes taken from different sources such as widely-used standard libraries and code written by students, we applied Daikon to infer contracts and compared the results (totaling more than 19500 inferred assertion clauses) with the already present contracts. We found that a contract inference tool can be used to strengthen programmer-written contracts, but cannot infer all contracts that humans write. The tool generates around five times as many relevant assertion clauses as written by programmers; but it only finds around 60\% of those originally written by programmers. Around a third of the generated assertions clauses are either incorrect or irrelevant. The study also uncovered interesting correlations between the quality of inferred contracts and some code metrics.},
	urldate = {2013-01-28},
	booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
	publisher = {{ACM}},
	author = {Polikarpova, Nadia and Ciupa, Ilinca and Meyer, Bertrand},
	year = {2009},
	keywords = {dynamic contract inference, eiffel},
	pages = {93–104},
	annote = {This paper uses Daikon system to generate specifications automatically, and compares the results with the existing human written contracts. The result shows that although Daikon produced five times as many contracts as the human written ones, it only produce 60\% of the human written contracts. The other contracts generated are either irrelevant or incorrect. As a result, the paper suggests to use tools as a complement and strengthen the programmer-written contracts, other than using tools only.},
	file = {ACM Full Text PDF:/home/derek/.zotero/zotero/6srlskv9.default/zotero/storage/69WBUREE/Polikarpova et al. - 2009 - A comparative study of programmer-written and auto.pdf:application/pdf}
},

@inproceedings{xiao_automated_2012,
	address = {New York, {NY}, {USA}},
	series = {{FSE} '12},
	title = {Automated extraction of security policies from natural-language software documents},
	isbn = {978-1-4503-1614-9},
	url = {http://doi.acm.org/10.1145/2393596.2393608},
	doi = {10.1145/2393596.2393608},
	abstract = {Access Control Policies ({ACP)} specify which principals such as users have access to which resources. Ensuring the correctness and consistency of {ACPs} is crucial to prevent security vulnerabilities. However, in practice, {ACPs} are commonly written in Natural Language ({NL)} and buried in large documents such as requirements documents, not amenable for automated techniques to check for correctness and consistency. It is tedious to manually extract {ACPs} from these {NL} documents and validate {NL} functional requirements such as use cases against {ACPs} for detecting inconsistencies. To address these issues, we propose an approach, called {Text2Policy}, to automatically extract {ACPs} from {NL} software documents and resource-access information from {NL} scenario-based functional requirements. We conducted three evaluations on the collected {ACP} sentences from publicly available sources along with use cases from both open source and proprietary projects. The results show that {Text2Policy} effectively identifies {ACP} sentences with the precision of 88.7\% and the recall of 89.4\%, extracts {ACP} rules with the accuracy of 86.3\%, and extracts action steps with the accuracy of 81.9\%.},
	urldate = {2013-01-28},
	booktitle = {Proceedings of the {ACM} {SIGSOFT} 20th International Symposium on the Foundations of Software Engineering},
	publisher = {{ACM}},
	author = {Xiao, Xusheng and Paradkar, Amit and Thummalapenta, Suresh and Xie, Tao},
	year = {2012},
	keywords = {access control, natural language processing, requirements analysis},
	pages = {12:1–12:11},
	annote = {This paper presents an approach called {Text2Policy} to automatically extract Access Control Policies ({ACP)} from Natural Language ({NL)} requirement documents. The approach consists of two steps. First, identify {ACP} sentences. Second, generate {ACP} rules. Experiment results show effectiveness within both steps.},
	file = {ACM Full Text PDF:/home/derek/.zotero/zotero/6srlskv9.default/zotero/storage/G233H8R6/Xiao et al. - 2012 - Automated extraction of security policies from nat.pdf:application/pdf}
},

@inproceedings{nimmer_automatic_2002,
	address = {New York, {NY}, {USA}},
	series = {{ISSTA} '02},
	title = {Automatic generation of program specifications},
	isbn = {1-58113-562-9},
	url = {http://doi.acm.org/10.1145/566172.566213},
	doi = {10.1145/566172.566213},
	abstract = {Producing specifications by dynamic (runtime) analysis of program executions is potentially unsound, because the analyzed executions may not fully characterize all possible executions of the program. In practice, how accurate are the results of a dynamic analysis? This paper describes the results of an investigation into this question, determining how much specifications generalized from program runs must be changed in order to be verified by a static checker. Surprisingly, small test suites captured nearly all program behavior required by a specific type of static checking; the static checker guaranteed that the implementations satisfy the generated specifications, and ensured the absence of runtime exceptions. Measured against this verification task, the generated specifications scored over 90\% on precision, a measure of soundness, and on recall, a measure of {completeness.This} is a positive result for testing, because it suggests that dynamic analyses can capture all semantic information of interest for certain applications. The experimental results demonstrate that a specific technique, dynamic invariant detection, is effective at generating consistent, sufficient specifications for use by a static checker. Finally, the research shows that combining static and dynamic analyses over program specifications has benefits for users of each technique, guaranteeing soundness of the dynamic analysis and lessening the annotation burden for users of the static analysis.},
	urldate = {2013-01-28},
	booktitle = {Proceedings of the 2002 {ACM} {SIGSOFT} international symposium on Software testing and analysis},
	publisher = {{ACM}},
	author = {Nimmer, Jeremy W. and Ernst, Michael D.},
	year = {2002},
	pages = {229–239},
	annote = {This paper proposes a technique that dynamically instruments the program to get the invariant of the program, and infer the specification for the program. The author also uses a static checker to ensure the correctness of the specification. The experiment results show that the dynamic invariant detector can generate specifications precisely enough to express the semantic of the program.},
	file = {ACM Full Text PDF:/home/derek/.zotero/zotero/6srlskv9.default/zotero/storage/UD3ZXK8M/Nimmer and Ernst - 2002 - Automatic generation of program specifications.pdf:application/pdf}
},

@inproceedings{tan_/*icomment:_2007,
	address = {New York, {NY}, {USA}},
	series = {{SOSP} '07},
	title = {/*icomment: bugs or bad comments?*/},
	isbn = {978-1-59593-591-5},
	shorttitle = {/*icomment},
	url = {http://doi.acm.org/10.1145/1294261.1294276},
	doi = {10.1145/1294261.1294276},
	abstract = {Commenting source code has long been a common practice in software development. Compared to source code, comments are more direct, descriptive and easy-to-understand. Comments and sourcecode provide relatively redundant and independent information regarding a program's semantic behavior. As software evolves, they can easily grow out-of-sync, indicating two problems: (1) bugs -the source code does not follow the assumptions and requirements specified by correct program comments; (2) bad comments - comments that are inconsistent with correct code, which can confuse and mislead programmers to introduce bugs in subsequent versions. Unfortunately, as most comments are written in natural language, no solution has been proposed to automatically analyze commentsand detect inconsistencies between comments and source code. This paper takes the first step in automatically analyzing commentswritten in natural language to extract implicit program rulesand use these rules to automatically detect inconsistencies between comments and source code, indicating either bugs or bad comments. Our solution, {iComment}, combines Natural Language Processing({NLP)}, Machine Learning, Statistics and Program Analysis techniques to achieve these goals. We evaluate {iComment} on four large code bases: Linux, Mozilla, Wine and Apache. Our experimental results show that {iComment} automatically extracts 1832 rules from comments with 90.8-100\% accuracy and detects 60 comment-code inconsistencies, 33 newbugs and 27 bad comments, in the latest versions of the four programs. Nineteen of them (12 bugs and 7 bad comments) have already been confirmed by the corresponding developers while the others are currently being analyzed by the developers.},
	urldate = {2013-01-29},
	booktitle = {Proceedings of twenty-first {ACM} {SIGOPS} symposium on Operating systems principles},
	publisher = {{ACM}},
	author = {Tan, Lin and Yuan, Ding and Krishna, Gopal and Zhou, Yuanyuan},
	year = {2007},
	keywords = {comment analysis, natural language processing for software engineering, programming rules and static analysis},
	pages = {145–158},
	annote = {This is the first paper to analyze the natural language comments of the code, extract implicit rules, and detect inconsistencies between the comments and the codes. The author adopts Natural Language Processing ({NLP)}, Machine Learning, Statistics and Program Analysis technique to do these jobs automatically. Experiment on four large software bases shows that this approach is efficient.},
	file = {ACM Full Text PDF:/home/derek/.zotero/zotero/6srlskv9.default/zotero/storage/KJCDTE2D/Tan et al. - 2007 - icomment bugs or bad comments.pdf:application/pdf}
},

@inproceedings{wei_inferring_2011,
	address = {New York, {NY}, {USA}},
	series = {{ICSE} '11},
	title = {Inferring better contracts},
	isbn = {978-1-4503-0445-0},
	url = {http://doi.acm.org/10.1145/1985793.1985820},
	doi = {10.1145/1985793.1985820},
	abstract = {Considerable progress has been made towards automatic support for one of the principal techniques available to enhance program reliability: equipping programs with extensive contracts. The results of current contract inference tools are still often unsatisfactory in practice, especially for programmers who already apply some kind of basic Design by Contract discipline, since the inferred contracts tend to be simple assertions - the very ones that programmers find easy to write. We present new, completely automatic inference techniques and a supporting tool, which take advantage of the presence of simple programmer-written contracts in the code to infer sophisticated assertions, involving for example implication and universal quantification. Applied to a production library of classes covering standard data structures such as linked lists, arrays, stacks, queues and hash tables, the tool is able, entirely automatically, to infer 75\% of the complete contracts - contracts yielding the full formal specification of the classes - with very few redundant or irrelevant clauses.},
	urldate = {2013-01-28},
	booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Wei, Yi and Furia, Carlo A. and Kazmin, Nikolay and Meyer, Bertrand},
	year = {2011},
	keywords = {contract inference, data mining, invariants, random testing},
	pages = {191–200},
	annote = {Based on the existing simple programmer-written contracts in the code, this paper's approach can generate more sophisticated specifications in the form of assertions automatically. The generated contracts can include clause with universal quantification and implication. The author also implemented a tool call {AutoInfer} to demonstrate the technique. This tool also includes techniques like dynamic analysis and data mining.},
	file = {ACM Full Text PDF:/home/derek/.zotero/zotero/6srlskv9.default/zotero/storage/APSPTEI2/Wei et al. - 2011 - Inferring better contracts.pdf:application/pdf}
},

@inproceedings{pandita_inferring_2012,
	address = {Piscataway, {NJ}, {USA}},
	series = {{ICSE} 2012},
	title = {Inferring method specifications from natural language {API} descriptions},
	isbn = {978-1-4673-1067-3},
	url = {http://dl.acm.org/citation.cfm?id=2337223.2337319},
	abstract = {Application Programming Interface ({API)} documents are a typical way of describing legal usage of reusable software libraries, thus facilitating software reuse. However, even with such documents, developers often overlook some documents and build software systems that are inconsistent with the legal usage of those libraries. Existing software verification tools require formal specifications (such as code contracts), and therefore cannot directly verify the legal usage described in natural language text of {API} documents against the code using that library. However, in practice, most libraries do not come with formal specifications, thus hindering tool-based verification. To address this issue, we propose a novel approach to infer formal specifications from natural language text of {API} documents. Our evaluation results show that our approach achieves an average of 92\% precision and 93\% recall in identifying sentences that describe code contracts from more than 2500 sentences of {API} documents. Furthermore, our results show that our approach has an average 83\% accuracy in inferring specifications from over 1600 sentences describing code contracts.},
	urldate = {2013-01-27},
	booktitle = {Proceedings of the 2012 International Conference on Software Engineering},
	publisher = {{IEEE} Press},
	author = {Pandita, Rahul and Xiao, Xusheng and Zhong, Hao and Xie, Tao and Oney, Stephen and Paradkar, Amit},
	year = {2012},
	pages = {815–825},
	annote = {Almost all the Application Programming Interface ({API)} documents use natural language to express the {API} usage rules, and almost all the existing {API} legal usage verification tools only accept formal specification as their input. This paper presents a novel approach to infer the code contract for software libraries from the {API} documents. The code contract is a kind of formal specification, and can assist the existing software verification tools to verify the client code against the legal usage of the {API.}},
	file = {ACM Full Text PDF:/home/derek/.zotero/zotero/6srlskv9.default/zotero/storage/QTKJKPK5/Pandita et al. - 2012 - Inferring method specifications from natural langu.pdf:application/pdf}
},

@inproceedings{zhong_inferring_2009,
	address = {Washington, {DC}, {USA}},
	series = {{ASE} '09},
	title = {Inferring Resource Specifications from Natural Language {API} Documentation},
	isbn = {978-0-7695-3891-4},
	url = {http://dx.doi.org/10.1109/ASE.2009.94},
	doi = {10.1109/ASE.2009.94},
	abstract = {Typically, software libraries provide {API} documentation, through which developers can learn how to use libraries correctly. However, developers may still write code inconsistent with {API} documentation and thus introduce bugs, as existing research shows that many developers are reluctant to carefully read {API} documentation. To find those bugs, researchers have proposed various detection approaches based on known specifications. To mine specifications, many approaches have been proposed, and most of them rely on existing client code. Consequently, these mining approaches would fail to mine specifications when client code is not available. In this paper, we propose an approach, called {Doc2Spec}, that infers resource specifications from {API} documentation. For our approach, we implemented a tool and conducted an evaluation on Javadocs of five libraries. The results show that our approach infers various specifications with relatively high precisions, recalls, and F-scores. We further evaluated the usefulness of inferred specifications through detecting bugs in open source projects. The results show that specifications inferred by {Doc2Spec} are useful to detect real bugs in existing projects.},
	urldate = {2013-01-27},
	booktitle = {Proceedings of the 2009 {IEEE/ACM} International Conference on Automated Software Engineering},
	publisher = {{IEEE} Computer Society},
	author = {Zhong, Hao and Zhang, Lu and Xie, Tao and Mei, Hong},
	year = {2009},
	pages = {307–318},
	annote = {This paper proposes an approach to infer resource specifications in the form of automata from the {API} documents in natural language. The author adopts Natural Language Processing ({NLP)} technique as well as Named Entiry Recognition ({NER)} based on Hidden Markov Model ({HMM)} to facilitate their approach. They implement {Doc2Spec}, which only takes Javadoc as its input, to prototype their technique, and shows usefulness in detecting real bugs in existing projects.},
	file = {ACM Full Text PDF:/home/derek/.zotero/zotero/6srlskv9.default/zotero/storage/86V2TSUX/Zhong et al. - 2009 - Inferring Resource Specifications from Natural Lan.pdf:application/pdf}
},

@inproceedings{tan_tcomment:_2012,
	title = {{@tComment:} Testing Javadoc Comments to Detect Comment-Code Inconsistencies},
	shorttitle = {{@tComment}},
	doi = {10.1109/ICST.2012.106},
	abstract = {Code comments are important artifacts in software. Javadoc comments are widely used in Java for {API} specifications. {API} developers write Javadoc comments, and {API} users read these comments to understand the {API}, e.g., reading a Javadoc comment for a method instead of reading the method body. An inconsistency between the Javadoc comment and body for a method indicates either a fault in the body or, effectively, a fault in the comment that can mislead the method callers to introduce faults in their code. We present a novel approach, called {@TCOMMENT}, for testing Javadoc comments, specifically method properties about null values and related exceptions. Our approach consists of two components. The first component takes as input source files for a Java project and automatically analyzes the English text in Javadoc comments to infer a set of likely properties for a method in the files. The second component generates random tests for these methods, checks the inferred properties, and reports inconsistencies. We evaluated {@TCOMMENT} on seven open-source projects and found 29 inconsistencies between Javadoc comments and method bodies. We reported 16 of these inconsistencies, and 5 have already been confirmed and fixed by the developers.},
	booktitle = {2012 {IEEE} Fifth International Conference on Software Testing, Verification and Validation ({ICST)}},
	author = {Tan, Shin Hwei and Marinov, D. and Tan, Lin and Leavens, {G.T.}},
	month = apr,
	year = {2012},
	keywords = {{API} specification, application program interfaces, Arrays, comment analysis, comment-code inconsistency detection, Contracts, Educational institutions, English text analysis, formal specification, Java, Javadoc comment testing, libraries, natural language processing, program testing, random testing, software artifact, Synchronization, {@TCOMMENT}, test generation, Testing, text analysis},
	pages = {260 --269},
	annote = {This paper presents an novel approach called {@tComment} to check the consistency between the Java method comment and the Java method implementation. In this paper, {@tComment} only focuses on null values and exceptions. The author did experiments on seven open source projects, and found 29 inconsistencies.},
	file = {IEEE Xplore Abstract Record:/home/derek/.zotero/zotero/6srlskv9.default/zotero/storage/UDJEU7I3/articleDetails.html:text/html;IEEE Xplore Full Text PDF:/home/derek/.zotero/zotero/6srlskv9.default/zotero/storage/ZM86K6C3/Tan et al. - 2012 - @tComment Testing Javadoc Comments to Detect Comm.pdf:application/pdf}
},

@inproceedings{khalek_testera:_2011,
	title = {{TestEra:} A tool for testing Java programs using alloy specifications},
	shorttitle = {{TestEra}},
	doi = {10.1109/ASE.2011.6100137},
	abstract = {This tool paper presents an embodiment of {TestEra} - a framework developed in previous work for specification-based testing of Java programs. To test a Java method, {TestEra} uses the method's pre-condition specification to generate test inputs and the post-condition to check correctness of outputs. {TestEra} supports specifications written in Alloy - a first-order, declarative language based on relations - and uses the {SAT-based} back-end of the Alloy tool-set for systematic generation of test suites. Each test case is a {JUnit} test method, which performs three key steps: (1) initialization of pre-state, i.e., creation of inputs to the method under test; (2) invocation of the method; and (3) checking the correctness of post-state, i.e., checking the method output. The tool supports visualization of inputs and outputs as object graphs for graphical illustration of method behavior. {TestEra} is available for download to be used as a library or as an Eclipse plug-in.},
	booktitle = {2011 26th {IEEE/ACM} International Conference on Automated Software Engineering ({ASE)}},
	author = {Khalek, {S.A.} and Yang, Guowei and Zhang, Lingming and Marinov, D. and Khurshid, S.},
	month = nov,
	year = {2011},
	keywords = {alloy specification, Alloy tool-set, declarative language, Eclipse plug-in, graphical illustration, graphs, Java, Java program testing, {JUnit} test method, libraries, Metals, object graph, program testing, Receivers, {SAT-based} backend, Software engineering, specification-based testing, specification languages, Systematics, {TestEra}, Testing},
	pages = {608 --611},
	annote = {This is a tool paper, which illustrates {TestEra.} {TestEra} is a tool for testing java programs. It receives specially annotated java program using a first-order specification language Alloy, generates {JUnit} testing code, and runs the testing code to give a testing report.},
	file = {IEEE Xplore Abstract Record:/home/derek/.zotero/zotero/6srlskv9.default/zotero/storage/S9AAJDME/articleDetails.html:text/html;IEEE Xplore Full Text PDF:/home/derek/.zotero/zotero/6srlskv9.default/zotero/storage/AFXIKUZM/Khalek et al. - 2011 - TestEra A tool for testing Java programs using al.pdf:application/pdf}
},

@inproceedings{novick_why_2006,
	address = {New York, {NY}, {USA}},
	series = {{SIGDOC} '06},
	title = {Why don't people read the manual?},
	isbn = {1-59593-523-1},
	url = {http://doi.acm.org/10.1145/1166324.1166329},
	doi = {10.1145/1166324.1166329},
	abstract = {Few users of computer applications seek help from the documentation. This paper reports the results of an empirical study of why this is so and examines how, in real work, users solve their usability problems. Based on in-depth interviews with 25 subjects representing a varied cross-section of users, we find that users do avoid using both paper and online help systems. Few users have paper manuals for the most heavily used applications, but none complained about their lack. Online help is more likely to be consulted than paper manuals, but users are equally likely to report that they solve their problem by asking a colleague or experimenting on their own. Users cite difficulties in navigating the help systems, particularly difficulties in finding useful search terms, and disappointment in the level of explanation found.},
	urldate = {2013-01-28},
	booktitle = {Proceedings of the 24th annual {ACM} international conference on Design of communication},
	publisher = {{ACM}},
	author = {Novick, David G. and Ward, Karen},
	year = {2006},
	keywords = {manuals, online help, problem-solving, usability},
	pages = {11–18},
	annote = {This paper studies the reason for the fact that few people use the document to assist their use of computer applications. The author conducted an interview with a varied cross-section users. The result of the interview shows that people prefer to consult their colleague or experiment by themselves other than referring to paper or online document. The reason for this phenomenon is that the structure of the documents are hard for the users to navigate.},
	file = {ACM Full Text PDF:/home/derek/.zotero/zotero/6srlskv9.default/zotero/storage/47XB9TQC/Novick and Ward - 2006 - Why don't people read the manual.pdf:application/pdf}
}