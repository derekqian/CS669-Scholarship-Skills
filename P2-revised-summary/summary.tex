\documentclass[11pt,letterpaper,oneside]{article}
%\usepackage[top=0.5in,left=1in,right=1in,bottom=1in]{geometry}
\usepackage[top=0.5in,left=1.8in,right=1.8in,bottom=1in]{geometry}
\usepackage{fancyvrb}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{url}

\usepackage{setspace}
\doublespacing

\title{Summary of "Inferring Method Specifications from Natural Language API Descriptions''}
\author{Dejun Qian\\electronseu@gmail.com}

\date{}

\begin{document}
\maketitle

% the problem solved, the approach used, and the results
Pandita et al. \cite{bib:Pandita} address the problem of automating the verification process for software product, 
using the Application Programming Interface (API) documents written in natural language as the baseline.
Software reuse gives software developers the ability to build larger software products with higher quality in shorter time.
The reused software library is usually provided to the developers in the form of source code or binary executable,
along with some documents explaining how to use the library.
Reading and understanding the documents is important to correctly use the provided functionalities.
It is even the only way when no source code is available.
However, the developers easily overlook some documents,
and misuse some of the functions,
which may lead to potential inconsistency between the software product and the documents.
Many verification tools are developed to detect the inconsistency,
however these tools only accept formal specifications as their input.
They cannot understand the documents written in natural language.
Pandita et al. \cite{bib:Pandita} present an approach to fill the gap between what the existing verification tools need and what the document can provide.

The authors develop a technique to get the code contract from the library documents.
The technique can read the API documents,
and output the formal specification in the form of code contract which can be accepted by several verification tools.
The technique is composed of two parts: contract sentence selection and code contract generation.
These two parts can be further divided into five steps: parsing, pre-processing, text analysis, post-processing and code contract generation.
In the parsing step, the parser processes the API documents and extracts intermediate contents of the method description, 
such as summary description, argument description, return description, exception description and remark description. 
In the pre-processing step, the pre-processor takes the intermediate content, 
and performs three categories of processing: meta-data augmentation, noun boosting and programming constructs and jargon handling.
These first two steps are critical as they can improve the results of the text-analysis step.
In the text-analysis step, the authors develop a text analysis engine framework, 
and adopt an existing Parts-Of-Speech (POS) tagger to annotate POS tags for the input sentences.
Then, they use an Natural Language Processing (NLP) technique known shallow parser to classify the sentences into semantic templates,
based on the lexical tokens generated by the POS tagger.
The shallow parser also generates First-Order Logic (FOL) expressions after it has done the classification.
In the post-processing step, the paper does three types of semantic analysis: removing irrelevant modifiers in predicates, classifying predicates into a semantic class based on domain dictionaries, and augmenting expressions.
In the final step, by using predefined mapping from predicates to code contracts,
the code contract generator translates the FOL into code contracts.

The author designed a prototype to demonstrate the technique developed,
and conducted three experiments to evaluate the effectiveness of the technique. 
The experiment results show that this technique gives 92\% precision and 92\% recall toward the contract sentence detection,
and achieves 82\% of accuracy of code contract generation.

\bibliographystyle{acm}
\bibliography{reference}

\end{document}
